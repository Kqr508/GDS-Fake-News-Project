{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da det i Assignment 1 gav bedst mening at bruge ``pandas``, da filen er en csv som passer godt til en dataframe, og da ``pandas`` kommer med en nem måde at læse csv filen på, har jeg valgt her igen at bruge ``pandas`` til at læse og arbejde ned [``news_sample.csv``](https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                domain        type  \\\n",
       "0  141               awm.com  unreliable   \n",
       "1  256     beforeitsnews.com        fake   \n",
       "2  700           cnnnext.com  unreliable   \n",
       "3  768               awm.com  unreliable   \n",
       "4  791  bipartisanreport.com   clickbait   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://awm.com/church-congregation-brings-gift...   \n",
       "1  http://beforeitsnews.com/awakening-start-here/...   \n",
       "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3  When a rare shark was caught, scientists were ...   \n",
       "4  Donald Trump has the unnerving ability to abil...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title          authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "\n",
       "   keywords meta_keywords                                   meta_description  \\\n",
       "0       NaN          ['']                                                NaN   \n",
       "1       NaN          ['']                                                NaN   \n",
       "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
       "3       NaN          ['']                                                NaN   \n",
       "4       NaN          ['']                                                NaN   \n",
       "\n",
       "  tags  summary  \n",
       "0  NaN      NaN  \n",
       "1  NaN      NaN  \n",
       "2  NaN      NaN  \n",
       "3  NaN      NaN  \n",
       "4  NaN      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_sample = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv').drop(columns=['Unnamed: 0'])\n",
    "news_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opgaven er her ligesom i Assignment 1, at \"structure, process, clean it\". Jeg er ikke sikker på hvad structure og process refere til, så jeg har bare tænkt mig at cleane det.\n",
    "\n",
    "I cleaning processen fra Assignment 1 var opgaven at:\n",
    "- Lowercase teksten\n",
    "- Udskifte tal med ``<NUM>``\n",
    "- Udskifte datoer med ``<DATE>``\n",
    "- Udskifte emails med ``<EMAIL>``\n",
    "- Udskifte URLs med ``<URL>``\n",
    "- Fjerne ekstra white-space\n",
    "\n",
    "Python biblioteket ``cleantext`` har indbygget funktionalitet for mange af disse, men jeg er utilfreds med måden den udskifter tal på, samt så mangler den en måde at udskifte datoer på, så derfor vil jeg kun gøre delvist brug af dette bibliotekt. Dog har den også yderligere cleaning funktionaliteter som jeg vil gøre brug af.\n",
    "\n",
    "Den anbefalede rækkefølge for cleaning processen er:\n",
    "1. Lowercase teksten\n",
    "2. Udskifte URL med ``<URL>``\n",
    "3. Udskifte email med ``<EMAIL>``\n",
    "4. Udskifte tlf.nr. med ``<PHONE>``\n",
    "5. Udskifte datoer med ``<DATE>``\n",
    "6. Udskifte valuta symboler med ``<CUR>``\n",
    "7. Udfskifte tal med ``<NUM>``\n",
    "8. Fjerne ekstra white-space\n",
    "\n",
    "Dog vil jeg kun delvist gøre brug af denne rækkefølge dan ``cleantext`` kan gøre flere af disse på én gang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 234 GVjv n N <CUR> <CUR> webstite com <EMAIL> +<PHONE> <PHONE>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cleantext as clean\n",
    "\n",
    "test = \"1 234 GVjv n N $ £ webstite.com email@adRess.ku.dk +4512345678 12345678\"\n",
    "\n",
    "clean.clean(test, lower=0, no_urls=1, no_emails=1, no_phone_numbers=1, no_digits=0, no_punct=1, no_currency_symbols=1, replace_with_punct=' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleantext as clean\n",
    "import re\n",
    "\n",
    "# Converting to lowercase\n",
    "def lowercase(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "# Replacing URLs\n",
    "def sub_URL(text:str):\n",
    "    regex = r'(?:http[s]?://)?(?:www\\.)?[\\w]+\\.[a-z]{2,}[\\w#-_]*'\n",
    "    return re.sub(regex, ' <URL> ', text)\n",
    "\n",
    "# Replacing Emails\n",
    "def sub_EMAIL(text:str):\n",
    "    regex = r'[\\w._-]+@[\\w._-]+\\.[a-z]{2,}'\n",
    "    return re.sub(regex, ' <EMAIL> ', text)\n",
    "\n",
    "# Replace tlf. num. AND replace Valuta (and URL and Email)\n",
    "def sub_CUR_PHONE(text:str):\n",
    "    return clean.clean(text, lower=0, no_urls=1, no_emails=1, no_phone_numbers=1, no_numbers=0, no_digits=0, no_punct=0, no_currency_symbols=1)    \n",
    "\n",
    "# Replacing Dates\n",
    "def sub_DATE(text:str):\n",
    "    regex1 = r'(?:\\b\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}\\b)' # matcher 20-02-2025 og 20/02/2025 og 20.02.2025 og 2-20-2025 og 2/20/2025 og 2025-02-20 og 20/2/25 2/20/25\n",
    "    regex2 = r'(?:(?:(?:the )?(?:[123]\\d|\\d)(?:st|nd|rd|th)? (?:of )?)?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|sep|dec|mon|tue|wed|thu|fri|sat|sun)[a-z]*\\.?(?: (?:the )?\\d{1,4}(?:st|nd|rd|th)?,?))(?: (?:20|19)[\\d]{2}s?\\b)?'\n",
    "    regex3 = r'(?:(?:(?:the )?(?:[123]\\d|\\d)(?:st|nd|rd|th)? of )(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|sep|dec|mon|tue|wed|thu|fri|sat|sun)[a-z]*)'\n",
    "    regex4 = r'(?:20|19)[\\d]{2}s?\\b'\n",
    "    return re.sub(regex4, ' <DATE> ', re.sub(regex3, ' <DATE> ', re.sub(regex2, ' <DATE> ', re.sub(regex1, ' <DATE> ', text))))\n",
    "\n",
    "# Replacing Numbers\n",
    "def sub_NUM(text:str):\n",
    "    regex = r'\\d(?:\\d|\\.|\\,|st|nd|rd|th)*'\n",
    "    return re.sub(regex, ' <NUM> ', text)\n",
    "\n",
    "# Replacing Punctuation in words\n",
    "def sub_punct(text:str):\n",
    "    regex = r'[^\\w\\s](?=[a-z])'\n",
    "    return re.sub(regex, ' ', text)\n",
    "\n",
    "# rep URL, rep Email, rep tlf.nr., rep Numbers, del punct, rep Valuta\n",
    "def cleaning_module(text:str):\n",
    "    return clean.clean(text, lower=0, no_urls=1, no_emails=1, no_phone_numbers=1, no_numbers=1, no_punct=1, no_currency_symbols=1, replace_with_number='<NUM>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg samler funktionerne der udføre hver opgave seperat, i én funktion ``CleanText``, hvor hvert cleaning step udføres i det korrekte rækkefølge. Nedenfor køre funktionerne tidligere desto mere indekseret det er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sec <NUM> coordination a the director of the office of science and technology policy ostp in consultation with the assistant to the president for homeland security and counterterrorism and the director of the office of management and budget omb shall coordinate the development and implementation of federal government activities to prepare the nation for space weather events including the activities established in section <NUM> of this order and the recommendations of the national science and technology council nstc established by executive order <NUM> <DATE> establishment of the national science and technology council'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CleanText(text:str) -> str :\n",
    "    \"\"\"Returns a cleaned string.\n",
    "    \n",
    "    Args:\n",
    "        text (str) : A (raw) uncleaned text (document).\n",
    "    Returns:\n",
    "        str : The same text, but cleaned.\"\"\"\n",
    "    return cleaning_module(\n",
    "        sub_punct(\n",
    "            sub_NUM(\n",
    "                sub_DATE(\n",
    "                    sub_CUR_PHONE(\n",
    "                        sub_EMAIL(\n",
    "                            sub_URL(\n",
    "                                lowercase(\n",
    "                                    text\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Test\n",
    "CleanText(r'Sec. 3. Coordination. (a) The Director of the Office of Science and Technology Policy (OSTP), in consultation with the Assistant to the President for Homeland Security and Counterterrorism and the Director of the Office of Management and Budget (OMB), shall coordinate the development and implementation of Federal Government activities to prepare the Nation for space weather events, including the activities established in section 5 of this order and the recommendations of the National Science and Technology Council (NSTC), established by Executive Order 12881 of November 23, 1993 (Establishment of the National Science and Technology Council).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen ``CleanText`` operere på én string ad gangen, så dette betyder at den skal påføres (apply) på hver dokument (string/data point) i vores datasæt. Altså skal du bruge noget der iterere over hvert element i datasættet, eksempelvis et ``for`` loop eller ``<DATA FRAME>.apply(<FUNKTION>)`` fra panda biblioteket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definere en ny dataframe for den ikke-cleanede og det cleanede data, for nemmere databehandling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>sometimes the power of christmas will make you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>awakening of &lt;NUM&gt; strands of dna reconnecting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>never hike alone a &lt;DATE&gt; fan film usa | &lt;DATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>when a rare shark was caught scientists were l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>donald trump has the unnerving ability to abil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Prison for Rahm, God’s Work And Many Others\\n\\...</td>\n",
       "      <td>prison for rahm god s work and many others\\nhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4 Useful Items for Your Tiny Home\\n\\nHeadline:...</td>\n",
       "      <td>&lt;NUM&gt; useful items for your tiny home\\nheadlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Former CIA Director Michael Hayden said Thursd...</td>\n",
       "      <td>former cia director michael hayden said thursd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Antonio Sabato Jr. says Hollywood's liberal el...</td>\n",
       "      <td>antonio sabato jr says hollywood s liberal eli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Former U.S. President Bill Clinton on Monday c...</td>\n",
       "      <td>former u s president bill clinton on monday ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Original  \\\n",
       "0    Sometimes the power of Christmas will make you...   \n",
       "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2    Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3    When a rare shark was caught, scientists were ...   \n",
       "4    Donald Trump has the unnerving ability to abil...   \n",
       "..                                                 ...   \n",
       "245  Prison for Rahm, God’s Work And Many Others\\n\\...   \n",
       "246  4 Useful Items for Your Tiny Home\\n\\nHeadline:...   \n",
       "247  Former CIA Director Michael Hayden said Thursd...   \n",
       "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
       "249  Former U.S. President Bill Clinton on Monday c...   \n",
       "\n",
       "                                               Cleaned  \n",
       "0    sometimes the power of christmas will make you...  \n",
       "1    awakening of <NUM> strands of dna reconnecting...  \n",
       "2    never hike alone a <DATE> fan film usa | <DATE...  \n",
       "3    when a rare shark was caught scientists were l...  \n",
       "4    donald trump has the unnerving ability to abil...  \n",
       "..                                                 ...  \n",
       "245  prison for rahm god s work and many others\\nhe...  \n",
       "246  <NUM> useful items for your tiny home\\nheadlin...  \n",
       "247  former cia director michael hayden said thursd...  \n",
       "248  antonio sabato jr says hollywood s liberal eli...  \n",
       "249  former u s president bill clinton on monday ca...  \n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining dataframe for data manipulation\n",
    "sample_documents = news_sample[['content']].rename(columns={'content': 'Original'})\n",
    "\n",
    "# Adding cleaned column\n",
    "sample_documents['Cleaned'] = sample_documents['Original'].apply(CleanText)\n",
    "\n",
    "sample_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined vocab of all documents\n",
    "vocab_with_stopwords = set()\n",
    "\n",
    "# Get combined vocab from strings\n",
    "def get_vocab(tokens:list[str], vocab:set[str]):\n",
    "    vocab.update(set(tokens))\n",
    "\n",
    "# Calculating the vocab\n",
    "sample_documents['Cleaned'].apply(lambda x: get_vocab(x.split(), vocab_with_stopwords))\n",
    "\n",
    "\n",
    "# Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Calculating the vocab without stopwords\n",
    "vocab_without_stopwords = vocab_with_stopwords - english_stopwords\n",
    "\n",
    "print(vocab_without_stopwords.__len__(), vocab_with_stopwords.__len__(), english_stopwords.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>4970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>4902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>4718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mowed</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zion</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandonment</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15397 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "the           9494\n",
       "of            4970\n",
       "to            4902\n",
       "and           4718\n",
       "a             3418\n",
       "...            ...\n",
       "outing           1\n",
       "mowed            1\n",
       "zion             1\n",
       "abandonment      1\n",
       "ate              1\n",
       "\n",
       "[15397 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = pd.Series().value_counts()\n",
    "\n",
    "def get_vocab_freq(tokens:list[str]):\n",
    "    global vocabulary\n",
    "    vocabulary = vocabulary.add(pd.Series(tokens).value_counts(), fill_value=0)\n",
    "\n",
    "sample_documents['Cleaned'].apply(lambda x: get_vocab_freq(x.split()))\n",
    "\n",
    "vocabulary = vocabulary.to_frame().sort_values(ascending=False, by='count').astype(int)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "#cleaned_content = news_sample['content'].apply(CleanText)\n",
    "#cleaned_content\n",
    "\n",
    "words_list = [\"ll\", \"hi\", \"not\", \"so\",  \"Andy\",\"\"]  \n",
    "\n",
    "filtered_words = []\n",
    "for word in words_list:\n",
    "    if word not in english_stopwords:  \n",
    "        filtered_words.append(word)\n",
    "\n",
    "\n",
    "print(filtered_words) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
