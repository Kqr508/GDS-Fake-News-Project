{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da det i Assignment 1 gav bedst mening at bruge ``pandas``, da filen er en csv som passer godt til en dataframe, og da ``pandas`` kommer med en nem måde at læse csv filen på, har jeg valgt her igen at bruge ``pandas`` til at læse og arbejde ned [``news_sample.csv``](https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>political</td>\n",
       "      <td>Iran News Round Up</td>\n",
       "      <td>['plu', 'one', 'articl', 'googl', 'plu', 'than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake</td>\n",
       "      <td>The Cost Of The Best Senate Banking Committee ...</td>\n",
       "      <td>['cost', 'best', 'senat', 'bank', 'committe', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>satire</td>\n",
       "      <td>Man Awoken From 27-Year Coma Commits Suicide A...</td>\n",
       "      <td>['man', 'awoken', '&lt;num&gt;', 'year', 'coma', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reliable</td>\n",
       "      <td>Opening a Gateway for Girls to Enter the Compu...</td>\n",
       "      <td>['julia', 'geist', 'ask', 'draw', 'pictur', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>100 Compiled Studies on Vaccine Dangers – Infi...</td>\n",
       "      <td>['&lt;num&gt;', 'compil', 'studi', 'vaccin', 'danger...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                              title  \\\n",
       "0   political                                 Iran News Round Up   \n",
       "1        fake  The Cost Of The Best Senate Banking Committee ...   \n",
       "2      satire  Man Awoken From 27-Year Coma Commits Suicide A...   \n",
       "3    reliable  Opening a Gateway for Girls to Enter the Compu...   \n",
       "4  conspiracy  100 Compiled Studies on Vaccine Dangers – Infi...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  ['plu', 'one', 'articl', 'googl', 'plu', 'than...  \n",
       "1  ['cost', 'best', 'senat', 'bank', 'committe', ...  \n",
       "2  ['man', 'awoken', '<num>', 'year', 'coma', 'co...  \n",
       "3  ['julia', 'geist', 'ask', 'draw', 'pictur', 'c...  \n",
       "4  ['<num>', 'compil', 'studi', 'vaccin', 'danger...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_FILE = \"data.csv\"\n",
    "news_sample = pd.read_csv(DATA_FILE)\n",
    "#news_sample = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv').drop(columns=['Unnamed: 0'])\n",
    "news_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opgaven er her ligesom i Assignment 1, at \"structure, process, clean it\". Jeg er ikke sikker på hvad structure og process refere til, så jeg har bare tænkt mig at cleane det.\n",
    "\n",
    "I cleaning processen fra Assignment 1 var opgaven at:\n",
    "- Lowercase teksten\n",
    "- Udskifte tal med ``<NUM>``\n",
    "- Udskifte datoer med ``<DATE>``\n",
    "- Udskifte emails med ``<EMAIL>``\n",
    "- Udskifte URLs med ``<URL>``\n",
    "- Fjerne ekstra white-space\n",
    "\n",
    "Python biblioteket ``cleantext`` har indbygget funktionalitet for mange af disse, men jeg er utilfreds med måden den udskifter tal på, samt så mangler den en måde at udskifte datoer på, så derfor vil jeg kun gøre delvist brug af dette bibliotekt. Dog har den også yderligere cleaning funktionaliteter som jeg vil gøre brug af.\n",
    "\n",
    "Den anbefalede rækkefølge for cleaning processen er:\n",
    "1. Lowercase teksten\n",
    "2. Udskifte URL med ``<URL>``\n",
    "3. Udskifte email med ``<EMAIL>``\n",
    "4. Udskifte tlf.nr. med ``<PHONE>``\n",
    "5. Udskifte datoer med ``<DATE>``\n",
    "6. Udskifte valuta symboler med ``<CUR>``\n",
    "7. Udfskifte tal med ``<NUM>``\n",
    "8. Fjerne ekstra white-space\n",
    "\n",
    "Dog vil jeg kun delvist gøre brug af denne rækkefølge dan ``cleantext`` kan gøre flere af disse på én gang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleantext as clean\n",
    "import re\n",
    "\n",
    "# Converting to lowercase\n",
    "def lowercase(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "url_regex = re.compile(r'(?:http[s]?://)?(?:www\\.)?[\\w]+\\.[a-z]{2,}[\\w#-_]*')\n",
    "# Replacing URLs\n",
    "def sub_URL(text:str):\n",
    "    return url_regex.sub(' <URL> ', text)\n",
    "\n",
    "email_regex = re.compile(r'[\\w._-]+@[\\w._-]+\\.[a-z]{2,}')\n",
    "# Replacing Emails\n",
    "def sub_EMAIL(text:str):\n",
    "    return email_regex.sub(' <EMAIL> ', text)\n",
    "\n",
    "date1_regex = re.compile(r'(?:\\b\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}\\b)') # matcher 20-02-2025 og 20/02/2025 og 20.02.2025 og 2-20-2025 og 2/20/2025 og 2025-02-20 og 20/2/25 2/20/25\n",
    "date2_regex = re.compile(r'(?:(?:(?:the )?(?:[123]\\d|\\d)(?:st|nd|rd|th)? (?:of )?)?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|sep|dec|mon|tue|wed|thu|fri|sat|sun)[a-z]*\\.?(?: (?:the )?\\d{1,4}(?:st|nd|rd|th)?,?))(?: (?:20|19)[\\d]{2}s?\\b)?')\n",
    "date3_regex = re.compile(r'(?:(?:(?:the )?(?:[123]\\d|\\d)(?:st|nd|rd|th)? of )(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|sep|dec|mon|tue|wed|thu|fri|sat|sun)[a-z]*)')\n",
    "date4_regex = re.compile(r'(?:20|19)[\\d]{2}s?\\b')\n",
    "# Replacing Dates\n",
    "def sub_DATE(text:str):\n",
    "    return date4_regex.sub(' <DATE> ', date3_regex.sub(' <DATE> ', date2_regex.sub(' <DATE> ', date1_regex.sub(' <DATE> ', text))))\n",
    "\n",
    "num_regex = re.compile(r'\\d(?:\\d|\\.|\\,|st|nd|rd|th)*')\n",
    "# Replacing Numbers\n",
    "def sub_NUM(text:str):\n",
    "    return num_regex.sub(' <NUM> ', text)\n",
    "\n",
    "punct_regex = re.compile(r'[^\\w\\s](?=[a-z])')\n",
    "# Replacing Punctuation in words\n",
    "def sub_punct(text:str):\n",
    "    return punct_regex.sub(' ', text)\n",
    "\n",
    "# rep URL, rep Email, rep tlf.nr., rep Numbers, del punct, rep Valuta\n",
    "def cleaning_module(text:str):\n",
    "    return clean.normalize_whitespace(clean.remove_punct(clean.replace_currency_symbols(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg samler funktionerne der udføre hver opgave seperat, i én funktion ``CleanText``, hvor hvert cleaning step udføres i det korrekte rækkefølge. Nedenfor køre funktionerne tidligere desto mere indekseret det er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sec <NUM> coordination a the director of the office of science and technology policy ostp in consultation with the assistant to the president for homeland security and counterterrorism and the director of the office of management and budget omb shall coordinate the development and implementation of federal government activities to prepare the nation for space weather events including the activities established in section <NUM> of this order and the recommendations of the national science and technology council nstc established by executive order <NUM> <DATE> establishment of the national science and technology council'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CleanText(text:str) -> str :\n",
    "    \"\"\"Returns a cleaned string.\n",
    "    \n",
    "    Args:\n",
    "        text (str) : A (raw) uncleaned text (document).\n",
    "    Returns:\n",
    "        str : The same text, but cleaned.\"\"\"\n",
    "    #hvis teksten indholder NaNa, retuner \"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return cleaning_module(\n",
    "        sub_punct(\n",
    "            sub_NUM(\n",
    "                sub_DATE(\n",
    "                    sub_EMAIL(\n",
    "                        sub_URL(\n",
    "                            lowercase(\n",
    "                                text\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Test\n",
    "CleanText(r'Sec. 3. Coordination. (a) The Director of the Office of Science and Technology Policy (OSTP), in consultation with the Assistant to the President for Homeland Security and Counterterrorism and the Director of the Office of Management and Budget (OMB), shall coordinate the development and implementation of Federal Government activities to prepare the Nation for space weather events, including the activities established in section 5 of this order and the recommendations of the National Science and Technology Council (NSTC), established by Executive Order 12881 of November 23, 1993 (Establishment of the National Science and Technology Council).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen ``CleanText`` operere på én string ad gangen, så dette betyder at den skal påføres (apply) på hver dokument (string/data point) i vores datasæt. Altså skal du bruge noget der iterere over hvert element i datasættet, eksempelvis et ``for`` loop eller ``<DATA FRAME>.apply(<FUNKTION>)`` fra panda biblioteket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definere en ny dataframe for den ikke-cleanede og det cleanede data, for nemmere databehandling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['plu', 'one', 'articl', 'googl', 'plu', 'than...</td>\n",
       "      <td>plu one articl googl plu thank ali alfoneh ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cost', 'best', 'senat', 'bank', 'committe', ...</td>\n",
       "      <td>cost best senat bank committe jp morgan buy cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['man', 'awoken', '&lt;num&gt;', 'year', 'coma', 'co...</td>\n",
       "      <td>man awoken num&gt; year coma commit suicid learn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['julia', 'geist', 'ask', 'draw', 'pictur', 'c...</td>\n",
       "      <td>julia geist ask draw pictur comput scientist l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['&lt;num&gt;', 'compil', 'studi', 'vaccin', 'danger...</td>\n",
       "      <td>num&gt; compil studi vaccin danger activist post ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994983</th>\n",
       "      <td>['&lt;num&gt;', 'wire', '+', 'intervent', 'watch', '...</td>\n",
       "      <td>num&gt; wire + intervent watch | say anyon sign a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994984</th>\n",
       "      <td>['plu', 'one', 'articl', 'googl', 'plu', 'than...</td>\n",
       "      <td>plu one articl googl plu thank ali alfoneh ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994985</th>\n",
       "      <td>['china', 'russia', 'acquir', 'gold', 'dump', ...</td>\n",
       "      <td>china russia acquir gold dump us dollar evid c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994986</th>\n",
       "      <td>['liverpool', 'sign', 'sadio', 'mane', 'southa...</td>\n",
       "      <td>liverpool sign sadio mane southampton fan reac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994987</th>\n",
       "      <td>['cnn', 'promot', 'coup', 'venezuela', 'maduro...</td>\n",
       "      <td>cnn promot coup venezuela maduro monday presid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994988 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Original  \\\n",
       "0       ['plu', 'one', 'articl', 'googl', 'plu', 'than...   \n",
       "1       ['cost', 'best', 'senat', 'bank', 'committe', ...   \n",
       "2       ['man', 'awoken', '<num>', 'year', 'coma', 'co...   \n",
       "3       ['julia', 'geist', 'ask', 'draw', 'pictur', 'c...   \n",
       "4       ['<num>', 'compil', 'studi', 'vaccin', 'danger...   \n",
       "...                                                   ...   \n",
       "994983  ['<num>', 'wire', '+', 'intervent', 'watch', '...   \n",
       "994984  ['plu', 'one', 'articl', 'googl', 'plu', 'than...   \n",
       "994985  ['china', 'russia', 'acquir', 'gold', 'dump', ...   \n",
       "994986  ['liverpool', 'sign', 'sadio', 'mane', 'southa...   \n",
       "994987  ['cnn', 'promot', 'coup', 'venezuela', 'maduro...   \n",
       "\n",
       "                                                  Cleaned  \n",
       "0       plu one articl googl plu thank ali alfoneh ass...  \n",
       "1       cost best senat bank committe jp morgan buy cu...  \n",
       "2       man awoken num> year coma commit suicid learn ...  \n",
       "3       julia geist ask draw pictur comput scientist l...  \n",
       "4       num> compil studi vaccin danger activist post ...  \n",
       "...                                                   ...  \n",
       "994983  num> wire + intervent watch | say anyon sign a...  \n",
       "994984  plu one articl googl plu thank ali alfoneh ass...  \n",
       "994985  china russia acquir gold dump us dollar evid c...  \n",
       "994986  liverpool sign sadio mane southampton fan reac...  \n",
       "994987  cnn promot coup venezuela maduro monday presid...  \n",
       "\n",
       "[994988 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining dataframe for data manipulation\n",
    "#sample_documents = news_sample[['content']].rename(columns={'content': 'Original'})\n",
    "sample_documents = news_sample[['cleaned']].rename(columns={'cleaned': 'Original'})\n",
    "\n",
    "# Adding cleaned column\n",
    "sample_documents['Cleaned'] = sample_documents['Original'].apply(CleanText)\n",
    "\n",
    "sample_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beregner størrelsen af ordforrådet (antal unikke ord), ved brug at set-oprators. Jeg beregner det samlede ordforråd for alle dokumenter i datasættet. Først beregner jeg ordforrådet for det cleanede dataset med stopwords, hvorefter jeg laver et nyt sæt, der er sættet af alle ord i ``vocab_with_stopwords`` uden alle ord i sættet ``english_stopwords``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127553 1127412 198\n"
     ]
    }
   ],
   "source": [
    "# Combined vocab of all documents\n",
    "vocab_with_stopwords = set()\n",
    "\n",
    "# Get combined vocab from strings\n",
    "def get_vocab(tokens:list[str], vocab:set[str]):\n",
    "    vocab.update(set(tokens))\n",
    "\n",
    "# Calculating the vocab\n",
    "sample_documents['Cleaned'].apply(lambda x: get_vocab(x.split(), vocab_with_stopwords))\n",
    "\n",
    "\n",
    "# Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Calculating the vocab without stopwords\n",
    "vocab_without_stopwords = vocab_with_stopwords - english_stopwords\n",
    "\n",
    "print(vocab_with_stopwords.__len__(), vocab_without_stopwords.__len__(), english_stopwords.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effekten af at fjerne stopwords har en mindre indflydelse på ordforrådets længde end forventet, men dette giver mening da antallet af unikke stopord ikke er stor, og da ``vocab_with_stopwords`` er et sæt, altså hvert ord optræder kun én gang, kan der ikke fjernes flere ord fra ordforrådet end sættet ``english_stopwords`` er lang.\n",
    "\n",
    "Da vi skal beregne størrelsen af ordforrådet før og efter *stemming* antager vi at dette betyder at vi stopwords allerede er fjernet. Vi bruger ``PorterStemmer().stem`` fra ``nltk.stem.porter``, til at lave en nyt ordforråd (set) af ord der har fået påført *stemming*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115465\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Calculating the vocab without stopwords\n",
    "vocab_with_stemming = {PorterStemmer().stem(word) for word in vocab_without_stopwords}\n",
    "\n",
    "print(vocab_with_stemming.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette program opdeler et renset nyhedsdata-sæt i 80% træning, 10% validering og 10% test ved hjælp af StratifiedShuffleSplit. Det sikrer, at fordelingen af nyhedstyper bevares i alle datasæt, hvilket forbedrer modelens præcision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal NaN efter fjernelse: 8580\n",
      "Dataerne der brugt for triaaning, som svar til 80: 757769\n",
      "Dataerne der blive brugt for (validering+ test): 189443 rækker\n",
      "Dataerne der brugt for validering (10%): 94721 rækker\n",
      "Dataerne der blive brugt for test (10%): 94722 rækker\n"
     ]
    }
   ],
   "source": [
    "# Koden køres på rensede data, derfor skal tidligere opgaver være kørt for at denne del kan fungere.\n",
    "# Det kræver, at pakken installeres med: \"pip install scikit-learn\"\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#fejner NaNa i texten, fordi det give fejl, når skulle dele opgave toi træning og test modellen\n",
    "news_sample = news_sample.dropna(subset=['cleaned', 'type'])\n",
    "# Bekræft at NaN er fjernet\n",
    "print(\"Antal NaN efter fjernelse:\", news_sample.isna().sum().sum())\n",
    "\n",
    "\n",
    "# Vi bruge trining modeken på cleand data \n",
    "X= news_sample ['cleaned']\n",
    "y= news_sample ['type'] #Brug nyhedstype som labels\n",
    "#vi er nyskerige om, hvor mange % af hver nyhedstype\n",
    "#print(y.value_counts(normalize=True))\n",
    "\n",
    "# Opret StratifiedShuffleSplit objekt\n",
    "split= StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, temp_index in split.split(X, y):\n",
    "    X_train, X_temp = X.iloc[train_index], X.iloc[temp_index]\n",
    "    y_train, y_temp = y.iloc[train_index], y.iloc[temp_index]\n",
    "    \n",
    "\n",
    "# Størrelsen på splits\n",
    "print(f\"Dataerne der brugt for triaaning, som svar til 80: {len(X_train)}\")\n",
    "print(f\"Dataerne der blive brugt for (validering+ test): {len(X_temp)} rækker\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Opret StratifiedShuffleSplit objekt\n",
    "split= StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for val_index, test_index in split.split(X_temp, y_temp):\n",
    "    X_val, X_test = X_temp.iloc[val_index], X_temp.iloc[test_index]\n",
    "    y_val, y_test = y_temp.iloc[val_index], y_temp.iloc[test_index]\n",
    "\n",
    "# Størrelsen på splits\n",
    "print(f\"Dataerne der brugt for validering (10%): {len(X_val)} rækker\")\n",
    "print(f\"Dataerne der blive brugt for test (10%): {len(X_test)} rækker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(31357) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch newsproj\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mText Cleaning (1).ipynb\u001b[m\n",
      "\t\u001b[31mText Cleaning.ipynb\u001b[m\n",
      "\t\u001b[31mdata.csv\u001b[m\n",
      "\t\u001b[31mmyenv/\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
