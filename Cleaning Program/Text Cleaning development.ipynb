{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da det i Assignment 1 gav bedst mening at bruge ``pandas``, da filen er en csv som passer godt til en dataframe, og da ``pandas`` kommer med en nem måde at læse csv filen på, har jeg valgt her igen at bruge ``pandas`` til at læse og arbejde ned [``news_sample.csv``](https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                domain        type  \\\n",
       "0  141               awm.com  unreliable   \n",
       "1  256     beforeitsnews.com        fake   \n",
       "2  700           cnnnext.com  unreliable   \n",
       "3  768               awm.com  unreliable   \n",
       "4  791  bipartisanreport.com   clickbait   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://awm.com/church-congregation-brings-gift...   \n",
       "1  http://beforeitsnews.com/awakening-start-here/...   \n",
       "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3  When a rare shark was caught, scientists were ...   \n",
       "4  Donald Trump has the unnerving ability to abil...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title          authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "\n",
       "   keywords meta_keywords                                   meta_description  \\\n",
       "0       NaN          ['']                                                NaN   \n",
       "1       NaN          ['']                                                NaN   \n",
       "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
       "3       NaN          ['']                                                NaN   \n",
       "4       NaN          ['']                                                NaN   \n",
       "\n",
       "  tags  summary  \n",
       "0  NaN      NaN  \n",
       "1  NaN      NaN  \n",
       "2  NaN      NaN  \n",
       "3  NaN      NaN  \n",
       "4  NaN      NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_sample = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv').drop(columns=['Unnamed: 0'])\n",
    "news_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opgaven er her ligesom i Assignment 1, at \"structure, process, clean it\". Jeg er ikke sikker på hvad structure og process refere til, så jeg har bare tænkt mig at cleane det.\n",
    "\n",
    "I cleaning processen fra Assignment 1 var opgaven at:\n",
    "- Lowercase teksten\n",
    "- Udskifte tal med ``<NUM>``\n",
    "- Udskifte datoer med ``<DATE>``\n",
    "- Udskifte emails med ``<EMAIL>``\n",
    "- Udskifte URLs med ``<URL>``\n",
    "- Fjerne ekstra white-space\n",
    "\n",
    "Python biblioteket ``cleantext`` har indbygget funktionalitet for mange af disse, men jeg er utilfreds med måden den udskifter tal på, samt så mangler den en måde at udskifte datoer på, så derfor vil jeg kun gøre delvist brug af dette bibliotekt. Dog har den også yderligere cleaning funktionaliteter som jeg vil gøre brug af.\n",
    "\n",
    "Den anbefalede rækkefølge for cleaning processen er:\n",
    "1. Lowercase teksten\n",
    "2. Udskifte URL med ``<URL>``\n",
    "3. Udskifte email med ``<EMAIL>``\n",
    "4. Udskifte tlf.nr. med ``<PHONE>``\n",
    "5. Udskifte datoer med ``<DATE>``\n",
    "6. Udskifte valuta symboler med ``<CUR>``\n",
    "7. Udfskifte tal med ``<NUM>``\n",
    "8. Fjerne ekstra white-space\n",
    "\n",
    "Dog vil jeg kun delvist gøre brug af denne rækkefølge dan ``cleantext`` kan gøre flere af disse på én gang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import cleantext as clean\n",
    "import re\n",
    "\n",
    "# Converting to lowercase\n",
    "def lowercase(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "url_regex = re.compile(r'(?:http[s]?://)?(?:www\\.)?[\\w]+\\.[a-z]{2,}[\\w#-_]*')\n",
    "# Replacing URLs\n",
    "def sub_URL(text:str):\n",
    "    return url_regex.sub(' <URL> ', text)\n",
    "\n",
    "email_regex = re.compile(r'[\\w._-]+@[\\w._-]+\\.[a-z]{2,}')\n",
    "# Replacing Emails\n",
    "def sub_EMAIL(text:str):\n",
    "    return email_regex.sub(' <EMAIL> ', text)\n",
    "\n",
    "date1_regex = re.compile(r'(?:\\b\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}\\b)') # matcher 20-02-2025 og 20/02/2025 og 20.02.2025 og 2-20-2025 og 2/20/2025 og 2025-02-20 og 20/2/25 2/20/25\n",
    "date2_regex = re.compile(r'(?:(?:(?:the )?(?:[123]\\d|\\d)(?:st|nd|rd|th)? (?:of )?)?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|sep|dec|mon|tue|wed|thu|fri|sat|sun)[a-z]*\\.?(?: (?:the )?\\d{1,4}(?:st|nd|rd|th)?,?))(?: (?:20|19)[\\d]{2}s?\\b)?')\n",
    "date3_regex = re.compile(r'(?:(?:(?:the )?(?:[123]\\d|\\d)(?:st|nd|rd|th)? of )(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|sep|dec|mon|tue|wed|thu|fri|sat|sun)[a-z]*)')\n",
    "date4_regex = re.compile(r'(?:20|19)[\\d]{2}s?\\b')\n",
    "# Replacing Dates\n",
    "def sub_DATE(text:str):\n",
    "    return date4_regex.sub(' <DATE> ', date3_regex.sub(' <DATE> ', date2_regex.sub(' <DATE> ', date1_regex.sub(' <DATE> ', text))))\n",
    "\n",
    "num_regex = re.compile(r'\\d(?:\\d|\\.|\\,|st|nd|rd|th)*')\n",
    "# Replacing Numbers\n",
    "def sub_NUM(text:str):\n",
    "    return num_regex.sub(' <NUM> ', text)\n",
    "\n",
    "punct_regex = re.compile(r'[^\\w\\s](?=[a-z])')\n",
    "# Replacing Punctuation in words\n",
    "def sub_punct(text:str):\n",
    "    return punct_regex.sub(' ', text)\n",
    "\n",
    "# rep URL, rep Email, rep tlf.nr., rep Numbers, del punct, rep Valuta\n",
    "def cleaning_module(text:str):\n",
    "    return clean.normalize_whitespace(clean.remove_punct(clean.replace_currency_symbols(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg samler funktionerne der udføre hver opgave seperat, i én funktion ``CleanText``, hvor hvert cleaning step udføres i det korrekte rækkefølge. Nedenfor køre funktionerne tidligere desto mere indekseret det er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sec <NUM> coordination a the director of the office of science and technology policy ostp in consultation with the assistant to the president for homeland security and counterterrorism and the director of the office of management and budget omb shall coordinate the development and implementation of federal government activities to prepare the nation for space weather events including the activities established in section <NUM> of this order and the recommendations of the national science and technology council nstc established by executive order <NUM> <DATE> establishment of the national science and technology council'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CleanText(text:str) -> str :\n",
    "    \"\"\"Returns a cleaned string.\n",
    "    \n",
    "    Args:\n",
    "        text (str) : A (raw) uncleaned text (document).\n",
    "    Returns:\n",
    "        str : The same text, but cleaned.\"\"\"\n",
    "    return cleaning_module(\n",
    "        sub_punct(\n",
    "            sub_NUM(\n",
    "                sub_DATE(\n",
    "                    sub_EMAIL(\n",
    "                        sub_URL(\n",
    "                            lowercase(\n",
    "                                text\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Test\n",
    "CleanText(r'Sec. 3. Coordination. (a) The Director of the Office of Science and Technology Policy (OSTP), in consultation with the Assistant to the President for Homeland Security and Counterterrorism and the Director of the Office of Management and Budget (OMB), shall coordinate the development and implementation of Federal Government activities to prepare the Nation for space weather events, including the activities established in section 5 of this order and the recommendations of the National Science and Technology Council (NSTC), established by Executive Order 12881 of November 23, 1993 (Establishment of the National Science and Technology Council).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen ``CleanText`` operere på én string ad gangen, så dette betyder at den skal påføres (apply) på hver dokument (string/data point) i vores datasæt. Altså skal du bruge noget der iterere over hvert element i datasættet, eksempelvis et ``for`` loop eller ``<DATA FRAME>.apply(<FUNKTION>)`` fra panda biblioteket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definere en ny dataframe for den ikke-cleanede og det cleanede data, for nemmere databehandling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>sometimes the power of christmas will make you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>awakening of &lt;NUM&gt; strands of dna reconnecting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>never hike alone a &lt;DATE&gt; fan film usa | &lt;DATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>when a rare shark was caught scientists were l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>donald trump has the unnerving ability to abil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Prison for Rahm, God’s Work And Many Others\\n\\...</td>\n",
       "      <td>prison for rahm god s work and many others\\nhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4 Useful Items for Your Tiny Home\\n\\nHeadline:...</td>\n",
       "      <td>&lt;NUM&gt; useful items for your tiny home\\nheadlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Former CIA Director Michael Hayden said Thursd...</td>\n",
       "      <td>former cia director michael hayden said thursd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Antonio Sabato Jr. says Hollywood's liberal el...</td>\n",
       "      <td>antonio sabato jr says hollywood s liberal eli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Former U.S. President Bill Clinton on Monday c...</td>\n",
       "      <td>former u s president bill clinton on monday ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Original  \\\n",
       "0    Sometimes the power of Christmas will make you...   \n",
       "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2    Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3    When a rare shark was caught, scientists were ...   \n",
       "4    Donald Trump has the unnerving ability to abil...   \n",
       "..                                                 ...   \n",
       "245  Prison for Rahm, God’s Work And Many Others\\n\\...   \n",
       "246  4 Useful Items for Your Tiny Home\\n\\nHeadline:...   \n",
       "247  Former CIA Director Michael Hayden said Thursd...   \n",
       "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
       "249  Former U.S. President Bill Clinton on Monday c...   \n",
       "\n",
       "                                               Cleaned  \n",
       "0    sometimes the power of christmas will make you...  \n",
       "1    awakening of <NUM> strands of dna reconnecting...  \n",
       "2    never hike alone a <DATE> fan film usa | <DATE...  \n",
       "3    when a rare shark was caught scientists were l...  \n",
       "4    donald trump has the unnerving ability to abil...  \n",
       "..                                                 ...  \n",
       "245  prison for rahm god s work and many others\\nhe...  \n",
       "246  <NUM> useful items for your tiny home\\nheadlin...  \n",
       "247  former cia director michael hayden said thursd...  \n",
       "248  antonio sabato jr says hollywood s liberal eli...  \n",
       "249  former u s president bill clinton on monday ca...  \n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining dataframe for data manipulation\n",
    "sample_documents = news_sample[['content']].rename(columns={'content': 'Original'})\n",
    "\n",
    "# Adding cleaned column\n",
    "sample_documents['Cleaned'] = sample_documents['Original'].apply(CleanText)\n",
    "\n",
    "sample_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beregner størrelsen af ordforrådet (antal unikke ord), ved brug at set-oprators. Jeg beregner det samlede ordforråd for alle dokumenter i datasættet. Først beregner jeg ordforrådet for det cleanede dataset med stopwords, hvorefter jeg laver et nyt sæt, der er sættet af alle ord i ``vocab_with_stopwords`` uden alle ord i sættet ``english_stopwords``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15300 15153 198\n"
     ]
    }
   ],
   "source": [
    "# Combined vocab of all documents\n",
    "vocab_with_stopwords = set()\n",
    "\n",
    "# Get combined vocab from strings\n",
    "def get_vocab(tokens:list[str], vocab:set[str]):\n",
    "    vocab.update(set(tokens))\n",
    "\n",
    "# Calculating the vocab\n",
    "sample_documents['Cleaned'].apply(lambda x: get_vocab(x.split(), vocab_with_stopwords))\n",
    "\n",
    "\n",
    "# Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Calculating the vocab without stopwords\n",
    "vocab_without_stopwords = vocab_with_stopwords - english_stopwords\n",
    "\n",
    "print(vocab_with_stopwords.__len__(), vocab_without_stopwords.__len__(), english_stopwords.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effekten af at fjerne stopwords har en mindre indflydelse på ordforrådets længde end forventet, men dette giver mening da antallet af unikke stopord ikke er stor, og da ``vocab_with_stopwords`` er et sæt, altså hvert ord optræder kun én gang, kan der ikke fjernes flere ord fra ordforrådet end sættet ``english_stopwords`` er lang.\n",
    "\n",
    "Da vi skal beregne størrelsen af ordforrådet før og efter *stemming* antager vi at dette betyder at vi stopwords allerede er fjernet. Vi bruger ``PorterStemmer().stem`` fra ``nltk.stem.porter``, til at lave en nyt ordforråd (set) af ord der har fået påført *stemming*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9915\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Calculating the vocab without stopwords\n",
    "vocab_with_stemming = {PorterStemmer().stem(word) for word in vocab_without_stopwords}\n",
    "\n",
    "print(vocab_with_stemming.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efter stemming, ses den største reduktion i word count, med et fald på 5.238 ord, fra 15.153 unikke ord (uden stopwords) til 9.915 efter stemming. Ved eftertanke giver dette også mening, da der findes mange variationer af det samme ord, eksempelvis bøjning i tid, som betyder at man vil se at mængden af unikke ord der er mulige at fjerne, er langt større end mængden af stopord der kunne fjernes, altså -5.238 ord efter stemming versus -147 ord efter stopword removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 995K Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing pipeline. Bruger ``ray`` biblioteket for parallelization.\n",
    "\n",
    "Indlæser hele csv filen som en dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>political</td>\n",
       "      <td>Plus one article on Google Plus\\n\\n(Thanks to ...</td>\n",
       "      <td>Iran News Round Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake</td>\n",
       "      <td>The Cost Of The Best Senate Banking Committee ...</td>\n",
       "      <td>The Cost Of The Best Senate Banking Committee ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>satire</td>\n",
       "      <td>Man Awoken From 27-Year Coma Commits Suicide A...</td>\n",
       "      <td>Man Awoken From 27-Year Coma Commits Suicide A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reliable</td>\n",
       "      <td>WHEN Julia Geist was asked to draw a picture o...</td>\n",
       "      <td>Opening a Gateway for Girls to Enter the Compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>– 100 Compiled Studies on Vaccine Dangers (Act...</td>\n",
       "      <td>100 Compiled Studies on Vaccine Dangers – Infi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994995</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>By\\n\\n21WIRE + Intervention Watch | All we can...</td>\n",
       "      <td>KONY 2012 Archives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994996</th>\n",
       "      <td>political</td>\n",
       "      <td>Plus one article on Google Plus\\n\\n(Thanks to ...</td>\n",
       "      <td>Iran News Round Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994997</th>\n",
       "      <td>unknown</td>\n",
       "      <td>China and Russia are Acquiring Gold, Dumping U...</td>\n",
       "      <td>China and Russia are Acquiring Gold, Dumping U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994998</th>\n",
       "      <td>rumor</td>\n",
       "      <td>Liverpool have signed Sadio Mane from Southamp...</td>\n",
       "      <td>Twitter reacts as Liverpool sign Sadio Mane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994999</th>\n",
       "      <td>bias</td>\n",
       "      <td>CNN promoting coup in Venezuela - Maduro\\n\\nOn...</td>\n",
       "      <td>CNN promoting coup in Venezuela - Maduro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                            content  \\\n",
       "0        political  Plus one article on Google Plus\\n\\n(Thanks to ...   \n",
       "1             fake  The Cost Of The Best Senate Banking Committee ...   \n",
       "2           satire  Man Awoken From 27-Year Coma Commits Suicide A...   \n",
       "3         reliable  WHEN Julia Geist was asked to draw a picture o...   \n",
       "4       conspiracy  – 100 Compiled Studies on Vaccine Dangers (Act...   \n",
       "...            ...                                                ...   \n",
       "994995  conspiracy  By\\n\\n21WIRE + Intervention Watch | All we can...   \n",
       "994996   political  Plus one article on Google Plus\\n\\n(Thanks to ...   \n",
       "994997     unknown  China and Russia are Acquiring Gold, Dumping U...   \n",
       "994998       rumor  Liverpool have signed Sadio Mane from Southamp...   \n",
       "994999        bias  CNN promoting coup in Venezuela - Maduro\\n\\nOn...   \n",
       "\n",
       "                                                    title  \n",
       "0                                      Iran News Round Up  \n",
       "1       The Cost Of The Best Senate Banking Committee ...  \n",
       "2       Man Awoken From 27-Year Coma Commits Suicide A...  \n",
       "3       Opening a Gateway for Girls to Enter the Compu...  \n",
       "4       100 Compiled Studies on Vaccine Dangers – Infi...  \n",
       "...                                                   ...  \n",
       "994995                                 KONY 2012 Archives  \n",
       "994996                                 Iran News Round Up  \n",
       "994997  China and Russia are Acquiring Gold, Dumping U...  \n",
       "994998        Twitter reacts as Liverpool sign Sadio Mane  \n",
       "994999           CNN promoting coup in Venezuela - Maduro  \n",
       "\n",
       "[994988 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the csv file\n",
    "news_corpus = pd.read_csv('995,000_rows.csv',usecols=['type','content', 'title']).dropna(subset=['content'])\n",
    "news_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel dataprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyk\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>political</td>\n",
       "      <td>Plus one article on Google Plus\\n\\n(Thanks to ...</td>\n",
       "      <td>Iran News Round Up</td>\n",
       "      <td>[plu, one, articl, googl, plu, thank, ali, alf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake</td>\n",
       "      <td>The Cost Of The Best Senate Banking Committee ...</td>\n",
       "      <td>The Cost Of The Best Senate Banking Committee ...</td>\n",
       "      <td>[cost, best, senat, bank, committe, jp, morgan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>satire</td>\n",
       "      <td>Man Awoken From 27-Year Coma Commits Suicide A...</td>\n",
       "      <td>Man Awoken From 27-Year Coma Commits Suicide A...</td>\n",
       "      <td>[man, awoken, &lt;num&gt;, year, coma, commit, suici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reliable</td>\n",
       "      <td>WHEN Julia Geist was asked to draw a picture o...</td>\n",
       "      <td>Opening a Gateway for Girls to Enter the Compu...</td>\n",
       "      <td>[julia, geist, ask, draw, pictur, comput, scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>– 100 Compiled Studies on Vaccine Dangers (Act...</td>\n",
       "      <td>100 Compiled Studies on Vaccine Dangers – Infi...</td>\n",
       "      <td>[&lt;num&gt;, compil, studi, vaccin, danger, activis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994983</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>By\\n\\n21WIRE + Intervention Watch | All we can...</td>\n",
       "      <td>KONY 2012 Archives</td>\n",
       "      <td>[&lt;num&gt;, wire, +, intervent, watch, |, say, any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994984</th>\n",
       "      <td>political</td>\n",
       "      <td>Plus one article on Google Plus\\n\\n(Thanks to ...</td>\n",
       "      <td>Iran News Round Up</td>\n",
       "      <td>[plu, one, articl, googl, plu, thank, ali, alf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994985</th>\n",
       "      <td>unknown</td>\n",
       "      <td>China and Russia are Acquiring Gold, Dumping U...</td>\n",
       "      <td>China and Russia are Acquiring Gold, Dumping U...</td>\n",
       "      <td>[china, russia, acquir, gold, dump, us, dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994986</th>\n",
       "      <td>rumor</td>\n",
       "      <td>Liverpool have signed Sadio Mane from Southamp...</td>\n",
       "      <td>Twitter reacts as Liverpool sign Sadio Mane</td>\n",
       "      <td>[liverpool, sign, sadio, mane, southampton, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994987</th>\n",
       "      <td>bias</td>\n",
       "      <td>CNN promoting coup in Venezuela - Maduro\\n\\nOn...</td>\n",
       "      <td>CNN promoting coup in Venezuela - Maduro</td>\n",
       "      <td>[cnn, promot, coup, venezuela, maduro, monday,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994988 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                            content  \\\n",
       "0        political  Plus one article on Google Plus\\n\\n(Thanks to ...   \n",
       "1             fake  The Cost Of The Best Senate Banking Committee ...   \n",
       "2           satire  Man Awoken From 27-Year Coma Commits Suicide A...   \n",
       "3         reliable  WHEN Julia Geist was asked to draw a picture o...   \n",
       "4       conspiracy  – 100 Compiled Studies on Vaccine Dangers (Act...   \n",
       "...            ...                                                ...   \n",
       "994983  conspiracy  By\\n\\n21WIRE + Intervention Watch | All we can...   \n",
       "994984   political  Plus one article on Google Plus\\n\\n(Thanks to ...   \n",
       "994985     unknown  China and Russia are Acquiring Gold, Dumping U...   \n",
       "994986       rumor  Liverpool have signed Sadio Mane from Southamp...   \n",
       "994987        bias  CNN promoting coup in Venezuela - Maduro\\n\\nOn...   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                      Iran News Round Up   \n",
       "1       The Cost Of The Best Senate Banking Committee ...   \n",
       "2       Man Awoken From 27-Year Coma Commits Suicide A...   \n",
       "3       Opening a Gateway for Girls to Enter the Compu...   \n",
       "4       100 Compiled Studies on Vaccine Dangers – Infi...   \n",
       "...                                                   ...   \n",
       "994983                                 KONY 2012 Archives   \n",
       "994984                                 Iran News Round Up   \n",
       "994985  China and Russia are Acquiring Gold, Dumping U...   \n",
       "994986        Twitter reacts as Liverpool sign Sadio Mane   \n",
       "994987           CNN promoting coup in Venezuela - Maduro   \n",
       "\n",
       "                                                  cleaned  \n",
       "0       [plu, one, articl, googl, plu, thank, ali, alf...  \n",
       "1       [cost, best, senat, bank, committe, jp, morgan...  \n",
       "2       [man, awoken, <num>, year, coma, commit, suici...  \n",
       "3       [julia, geist, ask, draw, pictur, comput, scie...  \n",
       "4       [<num>, compil, studi, vaccin, danger, activis...  \n",
       "...                                                   ...  \n",
       "994983  [<num>, wire, +, intervent, watch, |, say, any...  \n",
       "994984  [plu, one, articl, googl, plu, thank, ali, alf...  \n",
       "994985  [china, russia, acquir, gold, dump, us, dollar...  \n",
       "994986  [liverpool, sign, sadio, mane, southampton, fa...  \n",
       "994987  [cnn, promot, coup, venezuela, maduro, monday,...  \n",
       "\n",
       "[994988 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "# initializes a stemmer and defines pipeline\n",
    "stemmer = PorterStemmer()\n",
    "def preprocessing_pipeline(text:str):\n",
    "    return [stemmer.stem(word) for word in CleanText(text).split() if word not in english_stopwords]\n",
    "\n",
    "# Del DataFrame i mindre bidder og udfør parallelt\n",
    "@ray.remote\n",
    "def apply_parallel(df_chunk:DataFrame):\n",
    "    df_chunk['cleaned'] = df_chunk['content'].apply(preprocessing_pipeline)\n",
    "    return df_chunk\n",
    "\n",
    "# Opdel DataFrame i bidder (f.eks. 4 bidder)\n",
    "num_splits = 24\n",
    "csv_parts = np.array_split(news_corpus, num_splits)\n",
    "\n",
    "# Udfør apply parallelt på hver bid\n",
    "futures = [apply_parallel.remote(split) for split in csv_parts]\n",
    "results = ray.get(futures)\n",
    "\n",
    "# Kombiner resultaterne til en DataFrame\n",
    "news_corpus = pd.concat(results, ignore_index=True)\n",
    "news_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laver det cleanede data om til en csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus.drop(columns=['content']).to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: Dette er forældet, og brugt til at udvikle pre-processing pipeline. Den reelle implementering af en cleaning program findes som ``Pre-Processing Program.py`` i mappen ``Cleaning Program``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
